# AI Cold Calling Agent

Ein KI-gestÃ¼tzter Agent fÃ¼r automatisierte Kaltakquise-Telefonate mit Emotionserkennung und kontinuierlichem Training.

## ğŸ¯ Projektziel

Entwicklung eines fortschrittlichen KI-Agenten fÃ¼r Kaltakquise-Telefonate, der:
- Kostenfrei auf einem Ubuntu 22 Server lÃ¤uft
- Leads und Termine generiert
- Sich kontinuierlich durch Trainingsdaten verbessert
- Emotionales GesprÃ¤chsverhalten erkennt und adaptiert
- **Flexible Auswahl zwischen lokalen und Cloud-basierten Sprach-APIs**

## ğŸ”‘ Hauptfunktionen

### Flexible Speech-to-Text (STT) Engines
Der Agent unterstÃ¼tzt mehrere STT-Engines, die per .env-Datei konfiguriert werden kÃ¶nnen:

- **Whisper** (OpenAI) - Lokal, kostenlos, GPU-beschleunigt
- **Deepgram** - Cloud API, Nova-2-Modell, hervorragende QualitÃ¤t fÃ¼r Deutsch
- **Azure Speech Services** - Enterprise-grade, zuverlÃ¤ssig
- **Google Cloud Speech** - Hochwertige Transkription mit niedrigen Latenzen

### Flexible Text-to-Speech (TTS) Engines
WÃ¤hlen Sie die optimale TTS-Engine fÃ¼r Ihre Anforderungen:

- **Coqui TTS** - Lokal, kostenlos, anpassbare Modelle
- **Mimic3** - Lokal, schnell, gute QualitÃ¤t
- **ElevenLabs** - Premium API, extrem natÃ¼rliche Stimmen
- **Azure Speech Services** - Professionelle Neural-Voices
- **Google Cloud TTS** - WaveNet und Neural2 Stimmen

### Weitere Funktionen
- **Telefonintegration**: Asterisk PBX Integration fÃ¼r professionelle Anrufverwaltung
- **Dialogsteuerung**: State-Machine + optionales lokales LLM (z.B. Ollama mit LLaMA-3, Mixtral)
- **Emotionserkennung**: Analyse und Anpassung des GesprÃ¤chsstils
- **Datenbank**: SQL-basierte Speicherung von:
  - GesprÃ¤chsleitfÃ¤den
  - FAQ und Antworten
  - Trainingsdaten
- **Dynamische Antwortgenerierung**: KI-gestÃ¼tzte Antworten bei fehlenden DatenbankeintrÃ¤gen

## ğŸ›  Technische Anforderungen

- Ubuntu Server 22.04 LTS
- Python 3.8+
- PostgreSQL/MySQL fÃ¼r Datenspeicherung
- Asterisk PBX fÃ¼r Telefonintegration (optional)
- Ausreichend Speicherplatz fÃ¼r Modelle und Trainingsdaten
- Optional: GPU fÃ¼r bessere Performance bei lokalen Modellen

## ğŸ“‚ Projektstruktur

```
â”œâ”€â”€ config/               # Konfigurationsdateien
â”œâ”€â”€ src/                 # Quellcode
â”‚   â”œâ”€â”€ database/       # Datenbankoperationen
â”‚   â”œâ”€â”€ speech/         # STT & TTS Module
â”‚   â”‚   â”œâ”€â”€ base.py              # Basis-Klassen fÃ¼r Engines
â”‚   â”‚   â”œâ”€â”€ stt.py               # Whisper STT
â”‚   â”‚   â”œâ”€â”€ stt_deepgram.py      # Deepgram STT
â”‚   â”‚   â”œâ”€â”€ stt_azure.py         # Azure STT
â”‚   â”‚   â”œâ”€â”€ stt_google.py        # Google Cloud STT
â”‚   â”‚   â”œâ”€â”€ tts.py               # Coqui & Mimic3 TTS
â”‚   â”‚   â”œâ”€â”€ tts_elevenlabs.py    # ElevenLabs TTS
â”‚   â”‚   â”œâ”€â”€ tts_azure.py         # Azure TTS
â”‚   â”‚   â””â”€â”€ tts_google.py        # Google Cloud TTS
â”‚   â”œâ”€â”€ conversation/   # GesprÃ¤chssteuerung
â”‚   â””â”€â”€ training/       # Trainingsmodule
â”œâ”€â”€ docs/                # Dokumentation
â”‚   â”œâ”€â”€ GERMAN_CALL_SCRIPTS.md  # Deutsche GesprÃ¤chsskripte
â”‚   â””â”€â”€ API.md                   # API-Dokumentation
â”œâ”€â”€ tests/               # Tests
â”‚   â””â”€â”€ test_voice_engines.py   # Voice-Engine Tests
â”œâ”€â”€ sql/                # SQL Schemas
â””â”€â”€ requirements.txt    # ProjektabhÃ¤ngigkeiten
```

## ğŸš€ Installation

```bash
# Repository klonen
git clone https://github.com/Pormetrixx/Aiagent.git
cd Aiagent

# Virtuelle Umgebung erstellen
python -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
.\venv\Scripts\activate  # Windows

# AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt
```

## ğŸ“ Konfiguration

### Konfigurationsmethoden

Das System unterstÃ¼tzt **zwei Konfigurationsmethoden**:

#### Option 1: Nur .env-Datei (Empfohlen fÃ¼r einfache Setups)

Die einfachste Methode ist die Verwendung nur der `.env`-Datei:

```bash
# .env-Datei aus Vorlage erstellen
cp .env.example .env

# .env-Datei bearbeiten und API-SchlÃ¼ssel eintragen
nano .env
```

**Das System funktioniert vollstÃ¤ndig mit nur der .env-Datei.** Die `config.yaml` ist optional!

#### Option 2: .env + config.yaml (Erweiterte Konfiguration)

FÃ¼r komplexere Setups kÃ¶nnen Sie zusÃ¤tzlich `config.yaml` verwenden:

```bash
# config.yaml aus Vorlage erstellen (optional)
cp config/config.example.yaml config/config.yaml

# Erweiterte Einstellungen bearbeiten
nano config/config.yaml
```

Die `.env`-Werte Ã¼berschreiben immer die `config.yaml`-Werte.

### 1. Umgebungsvariablen einrichten

```bash
# .env-Datei aus Vorlage erstellen
cp .env.example .env

# .env-Datei bearbeiten und API-SchlÃ¼ssel eintragen
nano .env
```

### 2. Speech-to-Text Engine auswÃ¤hlen

In `.env`:
```bash
# WÃ¤hlen Sie eine Engine: whisper, deepgram, azure, google
STT_ENGINE=whisper
STT_LANGUAGE=de

# FÃ¼r Whisper (lokal, kostenlos)
STT_WHISPER_MODEL_SIZE=base
STT_WHISPER_DEVICE=cpu

# FÃ¼r Deepgram (API, sehr gut fÃ¼r Deutsch)
DEEPGRAM_API_KEY=ihr_deepgram_api_key
STT_DEEPGRAM_MODEL=nova-2

# FÃ¼r Azure Speech
AZURE_SPEECH_KEY=ihr_azure_key
AZURE_SPEECH_REGION=westeurope

# FÃ¼r Google Cloud
GOOGLE_APPLICATION_CREDENTIALS=/pfad/zu/credentials.json
```

### 3. Text-to-Speech Engine auswÃ¤hlen

In `.env`:
```bash
# WÃ¤hlen Sie eine Engine: coqui, mimic3, elevenlabs, azure, google
TTS_ENGINE=coqui
TTS_LANGUAGE=de

# FÃ¼r Coqui TTS (lokal, kostenlos)
TTS_COQUI_MODEL=tts_models/de/thorsten/tacotron2-DDC
TTS_COQUI_DEVICE=cpu

# FÃ¼r ElevenLabs (Premium-QualitÃ¤t)
ELEVENLABS_API_KEY=ihr_elevenlabs_key
TTS_ELEVENLABS_VOICE_ID=voice_id_fÃ¼r_deutsche_stimme

# FÃ¼r Azure TTS (professionell)
TTS_AZURE_VOICE=de-DE-KatjaNeural

# FÃ¼r Google Cloud TTS
TTS_GOOGLE_VOICE=de-DE-Wavenet-C
```

**Hinweis:** Sie benÃ¶tigen **keine** `config.yaml` Datei! Alle Einstellungen kÃ¶nnen Ã¼ber die `.env`-Datei vorgenommen werden. Das System verwendet Standardwerte fÃ¼r nicht gesetzte Optionen.

### 4. Empfohlene Stimmen fÃ¼r Investment Cold Calling (Deutsch)

#### Lokale Engines (kostenlos):
- **Coqui TTS**: `thorsten` (mÃ¤nnlich, professionell)
- **Mimic3**: `de_DE/thorsten_low` (mÃ¤nnlich, klar)

#### Cloud Engines (professionell):
- **Azure**: 
  - `de-DE-KatjaNeural` (weiblich, freundlich)
  - `de-DE-ConradNeural` (mÃ¤nnlich, autoritativ)
- **Google Cloud**:
  - `de-DE-Wavenet-C` (weiblich, natÃ¼rlich)
  - `de-DE-Neural2-B` (mÃ¤nnlich, professionell)
- **ElevenLabs**: Custom Voice Cloning empfohlen fÃ¼r Markenkonsistenz

### 5. Konfigurationsdatei anpassen (optional)

FÃ¼r erweiterte Einstellungen:
```bash
# config.yaml aus Vorlage erstellen
cp config/config.example.yaml config/config.yaml

# Konfiguration bearbeiten
nano config/config.yaml
```

## ğŸ§ª Testen der Voice Engines

FÃ¼hren Sie das Test-Script aus, um alle konfigurierten Engines zu Ã¼berprÃ¼fen:

```bash
python tests/test_voice_engines.py
```

Das Script:
- ÃœberprÃ¼ft die VerfÃ¼gbarkeit aller Engines
- Testet STT mit Beispiel-Audio (falls vorhanden)
- Testet TTS und erstellt Audio-Samples
- Zeigt eine Zusammenfassung aller Tests

## ğŸ¤ GesprÃ¤chsskripte

Deutsche GesprÃ¤chsskripte fÃ¼r Investment Lead Generation finden Sie in:
`docs/GERMAN_CALL_SCRIPTS.md`

Diese beinhalten:
- ErÃ¶ffnungsszenarien
- Qualifizierungsfragen
- Einwandbehandlung
- Nutzenargumentation
- Abschluss-Techniken
- Voicemail-Skripte

## ğŸ’¡ API-SchlÃ¼ssel und Credits

### Kostenlose/Lokale Optionen:
- **Whisper**: VollstÃ¤ndig kostenlos, lÃ¤uft lokal
- **Coqui TTS**: VollstÃ¤ndig kostenlos, lÃ¤uft lokal
- **Mimic3**: VollstÃ¤ndig kostenlos, lÃ¤uft auf lokalem Server

### Cloud APIs mit kostenlosen Kontingenten:
- **Deepgram**: $200 Free Credits, dann $0.0043/Minute
- **Azure Speech**: 5 Std/Monat gratis, dann ab â‚¬0.84/Std
- **Google Cloud**: $300 Credits fÃ¼r neue Konten, dann ab $0.006/15 Sek

### Premium APIs:
- **ElevenLabs**: Ab $5/Monat fÃ¼r 30.000 Zeichen

## ğŸ” API-SchlÃ¼ssel erhalten

### Deepgram:
1. Registrieren auf https://console.deepgram.com/
2. API-SchlÃ¼ssel erstellen
3. Nova-2 Model fÃ¼r beste deutsche Spracherkennung verwenden

### Azure Speech Services:
1. Azure-Konto erstellen: https://portal.azure.com/
2. "Speech Services" Ressource erstellen
3. API-SchlÃ¼ssel und Region kopieren

### Google Cloud:
1. GCP-Konto erstellen: https://console.cloud.google.com/
2. "Speech-to-Text" und "Text-to-Speech" APIs aktivieren
3. Service-Account erstellen und JSON-Credentials herunterladen

### ElevenLabs:
1. Registrieren auf https://elevenlabs.io/
2. API-SchlÃ¼ssel im Dashboard generieren
3. Deutsche Stimmen auswÃ¤hlen oder Voice Cloning nutzen

## ğŸƒ System starten

```bash
# Agent starten
python3 run.py start

# Mit custom config
python3 run.py start -c /pfad/zu/config.yaml

# Als Systemdienst (nach setup.sh)
sudo systemctl start aiagent
```

## ğŸ“Š Performance-Empfehlungen

### FÃ¼r optimale Latenz:
- **STT**: Deepgram Nova-2 (schnellste Cloud-LÃ¶sung)
- **TTS**: Azure Neural Voices (niedrige Latenz)

### FÃ¼r beste QualitÃ¤t:
- **STT**: Google Cloud (hÃ¶chste Genauigkeit)
- **TTS**: ElevenLabs (natÃ¼rlichste Stimmen)

### FÃ¼r Kosteneffizienz:
- **STT**: Whisper lokal (mit GPU fÃ¼r gute Performance)
- **TTS**: Coqui TTS lokal

### FÃ¼r Datenschutz (On-Premise):
- **STT**: Whisper lokal
- **TTS**: Coqui TTS oder Mimic3 lokal

## ğŸ”§ Troubleshooting

Siehe `SETUP.md` fÃ¼r detaillierte Installationsanweisungen und Fehlerbehebung.

### HÃ¤ufige Probleme:

1. **Engine nicht verfÃ¼gbar**: API-SchlÃ¼ssel Ã¼berprÃ¼fen
2. **Schlechte Audio-QualitÃ¤t**: Sample-Rate und Codec prÃ¼fen
3. **Hohe Latenz**: Netzwerkverbindung oder lokale Engine nutzen
4. **API-Limits erreicht**: Zu kostenlosem Tier oder lokalem Model wechseln

## ğŸ¤ Beitragen

BeitrÃ¤ge sind willkommen! Bitte erstellen Sie einen Issue oder Pull Request.

### Neue Engines hinzufÃ¼gen:

1. Erstellen Sie eine neue Engine-Klasse (erbt von `BaseSTTEngine` oder `BaseTTSEngine`)
2. Implementieren Sie die erforderlichen Methoden
3. FÃ¼gen Sie die Engine zur Factory-Funktion hinzu
4. Aktualisieren Sie `.env.example` und Dokumentation
5. FÃ¼gen Sie Tests hinzu

## ğŸ“„ Lizenz

Dieses Projekt ist unter der MIT-Lizenz lizenziert.

## ğŸŒŸ Features Roadmap

- [ ] OpenAI Whisper API Integration
- [ ] OpenVoice TTS Integration
- [ ] Streaming-Audio-Support fÃ¼r alle Engines
- [ ] Automatic Engine Fallback bei AusfÃ¤llen
- [ ] Voice Activity Detection (VAD) Integration
- [ ] Multi-Language Support Ã¼ber alle Engines
- [ ] Performance Benchmarks fÃ¼r alle Engines
